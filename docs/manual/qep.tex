%-------------------------------------------------------
% SLEPc Users Manual
%-------------------------------------------------------
\chapter{\label{cap:qep}QEP: Quadratic Eigenvalue Problems}
%-------------------------------------------------------

\noindent The Quadratic Eigenvalue Problem (\ident{QEP}) solver object is intended for addressing polynomial eigenproblems of degree 2. Apart from specific quadratic solvers, it provides the possibility of reducing the problem to a generalized eigenvalue problem via linearization, then solving it with the \ident{EPS} package.

As in the case of \ident{SVD}, the user interface of \ident{QEP} is very similar to \ident{EPS}. We highlight the main differences both in usage and theoretical properties.

\section{\label{sec:qep}Overview of Quadratic Eigenproblems}

In this section, we review some basic properties of quadratic eigenvalue problems. The main goal is to set up the notation as well as describe the linearization approaches that will be employed for solving via the \ident{EPS} object. For additional background material about the quadratic eigenproblem, the reader is referred to \citep{Tisseur:2001:QEP}. As always, some details of the implemented methods can be found in the \slepc \hyperlink{str}{technical reports}.

In many applications, e.g., problems arising from second-order differential equations such as the analysis of damped vibrating systems, the eigenproblem to be solved is quadratic,
\begin{equation}
(\lambda^2M+\lambda C+K)x=0,\label{eq:eigquad}
\end{equation}
where $M,C,K\in\mathbb{C}^{n\times n}$ are the coefficients of a matrix polynomial of degree 2, $\lambda\in\mathbb{C}$ is the eigenvalue and $x\in\mathbb{C}^n$ is the eigenvector. As in the case of linear eigenproblems, the eigenvalues and eigenvectors can be complex even in the case that all three matrices are real.

It is important to point out some outstanding differences with respect to the linear eigenproblem. In the quadratic eigenproblem, the number of eigenvalues is $2n$, and the eigenvectors are not guaranteed to be linearly independent. If $M$ is singular, some eigenvalues may be infinite. Even when the three matrices are symmetric and positive definite, there is no guarantee that the eigenvalues are real, but still methods can exploit symmetry to some extent. Furthermore, numerical difficulties are more likely than in the linear case, so the computed solution can sometimes be untrustworthy.

If Eq.\ \ref{eq:eigquad} is written as $Q(\lambda)x=0$, where $Q$ is the matrix polynomial, then multiplication by $\lambda^{-2}$ results in $R(\lambda^{-1})x=0$, where $R$ is a matrix polynomial with the coefficients in the reverse order. In other words, if a method is available for computing the largest eigenvalues, then reversing the roles of $M$ and $K$ results in the computation of the smallest eigenvalues. In general, it is also possible to formulate different spectral trasformation for computing eigenvalues closest to a given target.

\paragraph{Equivalent Eigenvalue Problems.}

It is possible to transform the quadratic eigenvalue problem to a linear generalized eigenproblem $Az=\lambda Bz$ (linearization) by doubling the order of the system, i.e., $A,B\in\mathbb{C}^{2n\times 2n}$. There are many ways of doing this, and all of them are based on defining the eigenvector of the linear problem as 
\begin{equation}
\label{eq:linevec}
z=\left[\begin{array}{c}x\\\lambda x\end{array}\right],
\end{equation}
or a similar way. Below, we show some of the most common linearizations.

\begin{itemize}
\item Non-symmetric linearizations. The resulting matrix pencil has no structure. Here, $\alpha$ is an optional scaling factor such as $\|M\|$ or $\|K\|$.
\begin{equation}
\label{eq:n1}
\mbox{N1:}\qquad
\left[\begin{array}{cc}0 & \alpha I\\-K & -C\end{array}\right]-\lambda\left[\begin{array}{cc}\alpha I & 0\\0 & M\end{array}\right]
\end{equation}
\medskip
\begin{equation}
\label{eq:n2}
\mbox{N2:}\qquad
\left[\begin{array}{cc}-K & 0\\0 & \alpha I\end{array}\right]-\lambda\left[\begin{array}{cc}C & M\\\alpha I & 0\end{array}\right]
\end{equation}

\medskip
\item Symmetric linearizations. If $M$, $C$, and $K$ are all symmetric (Hermitian), the resulting matrix pencil is symmetric (Hermitian), although indefinite.
\begin{equation}
\label{eq:s1}
\mbox{S1:}\qquad
\left[\begin{array}{cc}0 & -K\\-K & -C\end{array}\right]-\lambda\left[\begin{array}{cc}-K & 0\\0 & M\end{array}\right]
\end{equation}
\medskip
\begin{equation}
\label{eq:s2}
\mbox{S2:}\qquad
\left[\begin{array}{cc}-K & 0\\0 & M\end{array}\right]-\lambda\left[\begin{array}{cc}C & M\\ M & 0\end{array}\right]
\end{equation}

\medskip
\item Hamiltonian linearizations. If $M$ is symmetric positive-definite, $K$ is symmetric, and $C$ is skew-symmetric (e.g., in real gyroscopic systems), the resulting matrix pencil has a Hamiltonian structure, i.e., eigenvalues come in quadruples $(\lambda,\bar{\lambda},-\lambda,-\bar{\lambda})$. One of the matrices is Hamiltonian and the other is skew-Hamiltonian. The first form (H1) is recommended when $M$ is singular, whereas the second form (H2) is recommended when $K$ is singular.
\begin{equation}
\label{eq:h1}
\mbox{H1:}\qquad
\left[\begin{array}{cc}K & 0\\C & K\end{array}\right]-\lambda\left[\begin{array}{cc} 0 & K\\-M & 0\end{array}\right]
\end{equation}
\medskip
\begin{equation}
\label{eq:h2}
\mbox{H2:}\qquad
\left[\begin{array}{cc}0 & -K\\M & 0\end{array}\right]-\lambda\left[\begin{array}{cc}M & C\\ 0 & M\end{array}\right]
\end{equation}
\end{itemize}

We could also consider the \emph{reversed} forms, e.g., the reversed form of N2 is
\begin{equation}
\label{eq:n2r}
\mbox{N2-R:}\qquad
\left[\begin{array}{cc}-C & -M\\\alpha I & 0\end{array}\right]-\frac{1}{\lambda}\left[\begin{array}{cc}K & 0\\0 & \alpha I\end{array}\right],
\end{equation}
which is equivalent to the form N1 for the problem $R(\lambda^{-1})x=0$.

In \slepc, some solvers of the \ident{QEP} class are based on using one of the above linearizations for solving the quadratic eigenproblem. These solvers make use of linear eigensolvers from the \ident{EPS} package.

%---------------------------------------------------
\section{Basic Usage}

The user interface of the \ident{QEP} package is very similar to \ident{EPS}. We will focus mainly on the relevant differences.

A basic example code for solving a quadratic eigenproblem with \ident{QEP} is shown in Figure \ref{fig:ex-qep}. The required steps are the same as those described in chapter \ref{cap:eps} for the linear eigenproblem. The main difference is that no problem type needs to be defined. As always, the solver context is created with \ident{QEPCreate}. The three problem matrices are specified with \ident{QEPSetOperators}. The call to \ident{QEPSolve} invokes the actual solver. Then, the solution is retrived with \ident{QEPGetConverged} and \ident{QEPGetEigenpair}. Finally, \ident{QEPDestroy} destroys the object.

\begin{figure}
\begin{Verbatim}[fontsize=\small,numbers=left,numbersep=6pt,xleftmargin=15mm]
QEP         qep;       /*  eigensolver context  */
Mat         M, C, K;   /*  matrices of the QEP  */
Vec         xr, xi;    /*  eigenvector, x       */
PetscScalar kr, ki;    /*  eigenvalue, k        */
PetscInt    j, nconv;
PetscReal   error;

QEPCreate( PETSC_COMM_WORLD, &qep );
QEPSetOperators( qep, M, C, K );
QEPSetFromOptions( qep );
QEPSolve( qep );
QEPGetConverged( qep, &nconv );
for (j=0; j<nconv; j++) {
  QEPGetEigenpair( qep, j, &kr, &ki, xr, xi );
  QEPComputeRelativeError( qep, j, &error );
}
QEPDestroy( qep );
\end{Verbatim}
\caption{\label{fig:ex-qep}Example code for basic solution with \ident{QEP}.}
\end{figure}


%---------------------------------------------------
\section{Defining the Problem}


%---------------------------------------------------
\section{Selecting the Solver}


%---------------------------------------------------
\section{Retrieving the Solution}



\paragraph{Reliability of the Computed Solution.}

When solving the quadratic problem via linearization, an accurate solution of the generalized eigenproblem does not necessarily imply accuracy for the quadratic problem. In \citep{Tisseur:2000:BEC}, it is shown that in the case of N1 linearization (Eq.\ \ref{eq:n1}), a small backward error in the generalized eigenproblem guarantees a small backward error in the quadratic eigenproblem. However, this holds only if $M$, $C$ and $K$ have a similar norm.

When the norm of $M$, $C$ and $K$ vary widely, \cite{Tisseur:2000:BEC} recommends to solve the scaled problem, defined as 
\begin{equation}
(\mu^2M_\alpha+\mu C_\alpha+K)x=0,\label{eq:scaled}
\end{equation}
with $\mu=\lambda/\alpha$, $M_\alpha=\alpha^2M$ and $C_\alpha=\alpha C$, where $\alpha$ is a scaling factor. Ideally, $\alpha$ should be chosen in such a way that the norms of $M_\alpha$, $C_\alpha$ and $K$ have similar magnitude.


\paragraph{Controlling and Monitoring Convergence.}


